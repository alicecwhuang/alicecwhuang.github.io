#### HPS/Pl 122: Probability Evidence and Belief 

#### Second Paper Topics

Papers Due: Saturday March 23 (by e-mail to instructor) 

Word limit: 2000 words 

#### Topic 1: Updating Problems 

Arntzenius argues against conditionalization as a rational norm for belief change. Explain his arguments -- focusing on one or two 
of his problem cases rather than all of them. Are they persuasive? Why or why not?

#### Topic 2: Updating Theory 

Explain Greaves and Wallace's argument for conditionalization. Recently, [Miriam Schoenfield](http://www.miriamschoenfield.com/F/conditionalization-final.pdf) 
has argued that conditionalization does not in general maximize expected accuracy. Explain Schoenfield's argument. 
Is it persuasive? Why or why not? You may also want to consider Huttegger's more general argument for reflection here.  

#### Topic 3: Learning and Value Change 

We discussed in class how to update beliefs after a "learning" experience that involves a change in one's values (the scare quotes
are there to remind you that it is an open question whether such a thing constitutes a learning experience). In a recent paper, 
Dmitri Gallow takes up learning and value change (available [here](http://pitt.edu/~jdg83/publication/pdfs/lavc.pdf)). 
Explain Gallow's argument. Do you find it persuasive? Why or why not? 

#### Topic 4: The Problem of new hypotheses

We talked quite a bit about how Bayesians can handle an increasing hypothesis space -- i.e., what happens when we learn 
something that we did not consider in our formulation of the problem. There are several ways to make this more precise: 
i.e., what happens when we learn that our partition of the logical space was incorrect; what happens when a probability zero
event occurs. We have two articles on this, from the economics literature -- Karni and Ortoleva. Choose one of them. Explain the argument. Discuss whether we can use it to account for the problem of new hypotheses. Why or why not? 

#### Topic 5: Error Correction 

One of the problems for Bayesians Meacham raises is what he calls error-correction. Is it rational to update by 
Bayesian conditioning in the face of the possibility of error? I.e., if I make a mistake, and continue to apply Bayes' Rule
thereafter I might end up in a less accurate place than if I deviated from conditioning in order to correct for my earlier error.
Do you think this is a problem for Bayesians? Why or why not? If it is, how can we account for it? 

#### Topic 6: Model Selection 

There are many directions for a paper in this area. If you want to write on model selection, we should talk in office hours. 
Start from the Gelman article but you would have to do some extra research for this topic. 

#### Topic 7: The "Piranha" Problem (a more ambitious modeling project)

Info [here](https://statmodeling.stat.columbia.edu/2018/03/07/important-statistical-theory-research-project-perfect-stat-grad-students-ambitious-undergrads/). If you'd like to work on this, come to office hours. 


