---
layout: page
title: Research
---

I'm interested in the conceptual foundations of machine learning and the ethical, epistemic, and social implications of AI systems. I also use computational models to study opinion dynamics and collaboration. Below are some of my recent and ongoing projects:

### What Do We Really Want from Interpretability?

Interpretability is one of the most discussed topics in machine learning today, but what additional information do we actually need from a model, beyond its predictions? The answer depends on our goals. My research explores the conceptual landscape of interpretability, with several ongoing projects:

#### SSHRC Insight Development Grant

**Title:** *Beyond the Accuracy-Interpretability Tradeoff: Optimizing Human-AI Collaborations*

(With Chris Smeenk)

This project investigates what kinds of information, beyond predictions, can enhance human-AI team performance. We also draw lessons from physics to explore how we might establish the reliability of models we don't fully understand.

#### Caution Against Model Simplicity

I’m currently writing a paper arguing that simple, inherently interpretable models are not necessarily better at achieving our ethical goals, because functional form is not what is at issue.

#### Counterfactual Explanations and Contestability

(With Thomas Grote)

We examine whether counterfactual explanations genuinely empower individuals to contest automated decisions — and if so, how?

### Can Social Media Unpolarize Us?

While it is often believed that polarization dominates public discourse, some evidence suggests that society is not as polarized as it appears. There's also a puzzling correlation between opinions on unrelated topics (e.g., anti-climate action and pro-gun stances), suggesting that our multi-dimensional opinion space has collapsed into just two perceived dimensions.

I study how misperceptions of opinion space contribute to polarization, and whether dynamically configuring social connections could help restore a multi-dimensional opinion space.

### Other Works in Progress

<ol start="1">
<li> The illusion of ethical-epistemic tradeoff (draft available) </li>
<li> A paper on gaslighting (draft available) </li>
<li> A paper on rewards in science (draft available)</li>
<li> A paper on hypes (draft available) </li>
</ol>

<!---
**Academic Articles**

1. Landscapes and Bandits: A Unified Model of Functional and Demographic Diversity,   
**Philosophy of Science** (2024)
[PDF](research/Diversity_PHOS.pdf)

2. Track Records: A Cautionary Tale,   
**The British Journal for the Philosophy of Science** (forthcoming)
[PDF](https://www.journals.uchicago.edu/doi/10.1086/728459)


3. A Normative Comparison of Threshold Views,
**Synthese** (2022) [PDF](https://link-springer-com.myaccess.library.utoronto.ca/article/10.1007/s11229-022-03784-x)

**Work in Progress** 

<ol start="4">
  <li> The illusion of ethical-epistemic tradeoff (draft available) </li>
  <li> A paper on gaslighting (draft available) </li>
  <li> A paper on rewards in science (draft available)</li>
  <li> A paper on decision trees and interpretability (draft available)</li>
  <li> A paper on hypes (draft available) </li>
</ol> --> 


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

<!---
<ol start="3">
  <li>A paper on approximate coherence </li>
  <li>Norms, Stereotypes and Accuracy <a href="babic_nsa.pdf">PDF</a> </li>
  <li>Adaptive Burdens of Proof (e-mail for draft) </li>
  <li>Dynamic Epistemic Risk </li>
  <li>Testing for Discrimination and the Risk of Error </li>
  <li>Invariance, Entropy, and (Objective) Bayesianism </li>
</ol> 
<!---
**Overview**
  The overaching theme of my current research is **epistemic risk**. It consists of three principal components:
**Philosophy of science/formal epistemology**. I try to motivate and construct a general theory of epistemic risk in terms of alethic sensitivity to small changes in accuracy. This theory is currently developed within the epistemic utility framework, though I think of this as a starting point rather than a fundamental commitment. If you would like to learn more, see the paper entitled A Theory of Epistemic Risk. This project proposes a way of measuring the riskiness of a credence function and connects risk to measures of uncertainty. In particular, I show that under very general conditions epistemic risk is dual to information entropy. 
Currently, I am working on a project that extends considerations of epistemic risk to the updating of beliefs (Dynamic Epistemic Risk). I aim to show that we can establish an update rule by considering how an agent's attitudes to epistemic risk should change in response to different possible learning experiences. Roughly, if the answer is that attitudes to epistemic risk should change as little as possible, then the associated update rule is Bayes' Rule. 
I am also working on a project on chance and coherence for imperfect Bayesian agents (Assessment Reversal in Approximate Coherentism). I suggest that approximating coherence may not be an appropriate proxy for traditional (all or nothing) coherence because unlike the latter, approximating coherence is susceptible to misfortune. 
**Normative ethics**. I believe the theory of epistemic risk can fruitfully speak to several problems that have been articulated in the moral encroachment and normative dilemmas literature. In a joint project with Zoë Johnson-King (Moral Obligations and Epistemic Risk), we explore the relationship between moral obligations and attitudes to epistemic risk.
**Law and public policy**. This dimension of my research engages the emerging literature on algorithmic fairness and ethics in statistics and machine learning. I am interested in both the normative dimension of what constitutes fair AI/ML and the statistical engineering problem of how to construct fair learning algorithms. I am also interested in the empirical study of related public policy problems. Currently, I am working on applying the theory of epistemic risk to evaluate the pervasiveness of discrimination. In particular, in Testing for Discrimination and the Risk of Error, I defend a statistical test for discrimination grounded in attitudes to epistemic risk. Meanwhile, in Adaptive Burdens of Proof, I argue that many apparent paradoxes of proof involving statistical evidence arise because we assume (without justification) that legal decision makers must have one unique attitude to epistemic risk -- namely, neutrality.  -->
